{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Helper functions\n",
    "from tdparse.notebook_helper import write_json_data\n",
    "# Models\n",
    "from tdparse.models.tdparse import TDParse, TDParsePlus\n",
    "# Word Vector methods\n",
    "from tdparse.word_vectors import GloveCommonCrawl, PreTrained\n",
    "# Dependency Parser\n",
    "from tdparse import stanford_tools\n",
    "from tdparse.dependency_parsers import tweebo, stanford\n",
    "from tdparse import tokenisers\n",
    "# Sentiment lexicons\n",
    "from tdparse import lexicons\n",
    "# Get the data\n",
    "from tdparse.parsers import semeval_14, dong, election\n",
    "from tdparse.data_types import TargetCollection\n",
    "from tdparse.helper import read_config, full_path\n",
    "# Evaluation methods\n",
    "from tdparse.evaluation import evaluation_results, scores, get_results, \\\n",
    "                               save_results, combine_results, get_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all of the datasets\n",
    "youtubean_train = semeval_14(full_path(read_config('youtubean_train')))\n",
    "youtubean_test = semeval_14(full_path(read_config('youtubean_test')))\n",
    "semeval_14_rest_train = semeval_14(full_path(read_config('semeval_2014_rest_train')))\n",
    "semeval_14_lap_train = semeval_14(full_path(read_config('semeval_2014_lap_train')))\n",
    "semeval_14_rest_test = semeval_14(full_path(read_config('semeval_2014_rest_test')))\n",
    "semeval_14_lap_test = semeval_14(full_path(read_config('semeval_2014_lap_test')))\n",
    "\n",
    "dong_train = dong(full_path(read_config('dong_twit_train_data')))\n",
    "dong_test = dong(full_path(read_config('dong_twit_test_data')))\n",
    "election_train, election_test = election(full_path(read_config('election_folder_dir')))\n",
    "\n",
    "mitchel_train = semeval_14(full_path(read_config('mitchel_train')))\n",
    "mitchel_test = semeval_14(full_path(read_config('mitchel_test')))\n",
    "\n",
    "dataset_train_test = {'Mitchel' : (mitchel_train, mitchel_test),\n",
    "                      'YouTuBean' : (youtubean_train, youtubean_test),\n",
    "                      'Election Twitter' : (election_train, election_test),\n",
    "                      'Dong Twitter' : (dong_train, dong_test),\n",
    "                      'SemEval 14 Restaurant' : (semeval_14_rest_train, semeval_14_rest_test),\n",
    "                      'SemEval 14 Laptop' : (semeval_14_lap_train, semeval_14_lap_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vectors\n",
    "sswe_path = full_path(read_config('sswe_files')['vo_zhang'])\n",
    "sswe = PreTrained(sswe_path, name='sswe')\n",
    "glove_300 = GloveCommonCrawl(version=42)\n",
    "\n",
    "\n",
    "# Load the sentiment lexicons and remove all words that are not associated\n",
    "# to the Positive or Negative class.\n",
    "subset_cats = {'positive', 'negative'}\n",
    "mpqa_low = lexicons.Mpqa(subset_cats=subset_cats, lower=True)\n",
    "nrc_low = lexicons.NRC(subset_cats=subset_cats, lower=True)\n",
    "hu_liu_low = lexicons.HuLiu(subset_cats=subset_cats, lower=True)\n",
    "mpqa_huliu_low = lexicons.Lexicon.combine_lexicons(mpqa_low, hu_liu_low)\n",
    "all_three_low = lexicons.Lexicon.combine_lexicons(mpqa_huliu_low, nrc_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_predictions(train, test, dataset_name, model, word_vector, random_state,  \n",
    "                        c_file_path, word_vector_file_path, model_dir,\n",
    "                        sentiment_lexicon=None, result_file_path=None,\n",
    "                        re_write=True, save_raw_data=True):\n",
    "    if not re_write and result_file_path is not None:\n",
    "        results_df = get_results(result_file_path, dataset_name)\n",
    "        if save_raw_data and results_df is not None:\n",
    "            if get_raw_data(result_file_path, dataset_name, test):\n",
    "                return results_df\n",
    "        elif results_df is not None:\n",
    "            return results_df\n",
    "    # loading the data\n",
    "    data_train = train.data()\n",
    "    y_train = train.sentiment_data()\n",
    "    data_test = test.data()\n",
    "    y_test = test.sentiment_data()\n",
    "    \n",
    "    \n",
    "    # Finding the best C value for the model on this dataset\n",
    "    c_grid_params = {'word_vectors' : [word_vector], 'random_state' : random_state,\n",
    "                     'parsers' : [tweebo], 'tokenisers' : [tokenisers.ark_twokenize]}\n",
    "    if sentiment_lexicon is not None:\n",
    "        c_grid_params['senti_lexicons'] = [sentiment_lexicon]\n",
    "    best_c, c_scores = model.find_best_c(data_train, y_train, c_grid_params, \n",
    "                                         save_file=c_file_path, dataset_name=dataset_name, \n",
    "                                         re_write=False, n_jobs=7, cv=5)\n",
    "    # Search over the different word vectors given the best tokeniser\n",
    "    # and sentiment lexicon\n",
    "    word_vectors = [[sswe]]\n",
    "    word_vector_grid_params = {**c_grid_params}\n",
    "    word_vector_grid_params['C'] = [best_c]\n",
    "    word_vector_grid_params['word_vectors'] = word_vectors\n",
    "    import time\n",
    "    t = time.time()\n",
    "    best_word_vector = model.save_grid_search(data_train, y_train, word_vector_grid_params, \n",
    "                                              'word_vectors', dataset_name, word_vector_file_path, \n",
    "                                              re_write=False, n_jobs=5, cv=5)\n",
    "    print('{} {}'.format(best_word_vector, time.time() - t))\n",
    "    t = time.time()\n",
    "    # Word Vector is too large to multi-process\n",
    "    word_vectors.extend([[glove_300]])\n",
    "    best_word_vector = model.save_grid_search(data_train, y_train, word_vector_grid_params, \n",
    "                                              'word_vectors', dataset_name, word_vector_file_path, \n",
    "                                              re_write=False, n_jobs=1, cv=5)\n",
    "    print('{} {}'.format(best_word_vector, time.time() - t))\n",
    "    # Fitting and getting predictions from the model.\n",
    "    parameters = {'word_vector' : best_word_vector, 'random_state' : random_state, \n",
    "                  'C' : best_c, 'tokeniser' : tokenisers.ark_twokenize, 'parser' : tweebo}\n",
    "    print('Best parameters for dataset {} are: {}'.format(dataset_name, parameters))\n",
    "    if sentiment_lexicon is not None:\n",
    "        parameters['senti_lexicon'] = sentiment_lexicon\n",
    "    best_params = model.get_params(**parameters)\n",
    "    model.fit(data_train, y_train, params=best_params)\n",
    "    predicted_values = model.predict(data_test)\n",
    "    # Save the model to the model zoo\n",
    "    model_file_name = '{} {}'.format(model, dataset_name)\n",
    "    model_file_path = os.path.join(model_dir, model_file_name)\n",
    "    model.save_model(model_file_path, verbose=1)\n",
    "    # Return the results\n",
    "    if result_file_path is not None:\n",
    "        return evaluation_results(predicted_values, test, dataset_name, \n",
    "                                  file_name=result_file_path, \n",
    "                                  save_raw_data=save_raw_data, re_write=True)\n",
    "    else:\n",
    "        return evaluation_results(predicted_values, test, dataset_name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances of the models\n",
    "tdparse = TDParse()\n",
    "tdparse_plus = TDParsePlus()\n",
    "models = [tdparse, tdparse_plus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the result files\n",
    "result_folder = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'results', 'TDParse Models'))\n",
    "model_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'model zoo'))\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_result_files = ['TDParse.tsv', 'TDParsePlus.tsv']\n",
    "model_result_files = [os.path.join(result_folder, result_file) for result_file in model_result_files]\n",
    "C_result_files = ['TDParse C.json', 'TDParsePlus C.json']\n",
    "C_result_files = [os.path.join(result_folder, result_file) for result_file in C_result_files]\n",
    "word_vector_result_files = ['TDParse word vector.json', 'TDParsePlus word vector.json']\n",
    "word_vector_result_files = [os.path.join(result_folder, result_file) for result_file in word_vector_result_files]\n",
    "# Parameters for each model\n",
    "std_model_parameters = {'word_vector' : [sswe], 'random_state' : 42}\n",
    "all_senti_model_parameters = {**std_model_parameters, 'sentiment_lexicon' : all_three_low}\n",
    "model_parameters = [std_model_parameters, all_senti_model_parameters]\n",
    "# Combining parameters and result files\n",
    "parameters_files = list(zip(model_parameters, model_result_files, C_result_files, \n",
    "                            word_vector_result_files, [model_dir]*2))\n",
    "\n",
    "model_files = dict(zip(models, parameters_files))\n",
    "model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time_to_process = time.time()\n",
    "for dataset_name, train_test in dataset_train_test.items():\n",
    "    print('Processing dataset {}'.format(dataset_name))\n",
    "    train, test = train_test\n",
    "    for model, parameter_file_paths in model_files.items():\n",
    "        print('Processing model {}'.format(model))\n",
    "        params_files = parameter_file_paths\n",
    "        parameters = params_files[0]\n",
    "        result_file_path = params_files[1]\n",
    "        c_fp = params_files[2]\n",
    "        word_vectors_fp = params_files[3]\n",
    "        model_dir = params_files[4]\n",
    "        dataset_predictions(train, test, dataset_name, model, \n",
    "                            result_file_path=result_file_path,\n",
    "                            re_write=True, save_raw_data=True,\n",
    "                            c_file_path=c_fp,\n",
    "                            word_vector_file_path=word_vectors_fp, \n",
    "                            model_dir=model_dir,\n",
    "                            **parameters)\n",
    "\n",
    "time_to_process = time.time() - time_to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time it took to process all the datasets {{round(time_to_process / 3600, 2)}} hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45778418130344817"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_to_process/ 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
