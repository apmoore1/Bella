{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "import os\n",
    "import sys\n",
    "import random as rn\n",
    "rn.seed(42)\n",
    "\n",
    "sys.path.append(os.path.abspath(os.pardir))\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from tdparse.helper import read_config, full_path\n",
    "from tdparse.parsers import mitchel\n",
    "from tdparse.data_types import TargetCollection, Target\n",
    "from tdparse import write_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training and Test sets for the Mitchel et al. Dataset\n",
    "We show how we created the Training and Test sets for this dataset.\n",
    "\n",
    "The original Dataset can be downloaded from [here](http://www.m-mitchell.com/code/MitchellEtAl-13-OpenSentiment.tgz) and the accompying paper can be found [here](https://www.aclweb.org/anthology/D13-1171). As Mitchel et al. Evaluated their models of 10 fold cross validation they do not have one train, test set therefore we take one of their train, test folds combine it and split it into 70% train and 30% test, we then save the new train and test dataset in XML format that is of the same format as the [SemEval 2014](http://alt.qcri.org/semeval2014/task4/) datasets (we choose this dataset format as we found it the easiest to parse, use, understand and visually understand).\n",
    "\n",
    "First ensure the following has been done:\n",
    "1. Download the dataset and get a train and test split from the folder /en/10-fold (we used train.1 and test.1)\n",
    "2. Ensure in the [config.yaml](../config.yaml) file that the following values have the correct file paths:\n",
    "  1. mitchel_org_train = the file path to train.1\n",
    "  2. mitchel_org_test = the file path to test.1\n",
    "  3. mitchel_train = the file path that you would like the new training dataset to go\n",
    "  4. mitchel_test = the file path that you would like the new test dataset to go\n",
    "\n",
    "The original dataset contains 3288 targets as stated in the paper. We also show in this notebook that we also get the same number of targets and thus have parsed the dataset correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mitchel Dataset\n",
    "mitchel_org_train = mitchel(full_path(read_config('mitchel_org_train')))\n",
    "mitchel_org_test = mitchel(full_path(read_config('mitchel_org_test')))\n",
    "\n",
    "mitchel_combined = TargetCollection.combine_collections(mitchel_org_train, \n",
    "                                                        mitchel_org_test)\n",
    "m_dataset_size = len(mitchel_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "m_dataset_size": "3288"
    }
   },
   "source": [
    "Parsed dataset size = {{m_dataset_size}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "\n",
    "mitchel_data = np.asarray(mitchel_combined.data_dict())\n",
    "mitchel_sentiment = np.asarray(mitchel_combined.sentiment_data())\n",
    "for train_indexs, test_indexs in splitter.split(mitchel_data, mitchel_sentiment):\n",
    "    train_data = mitchel_data[train_indexs]\n",
    "    test_data = mitchel_data[test_indexs]\n",
    "    \n",
    "convert_to_targets = lambda data: [Target(**target) for target in data]\n",
    "mitchel_train = TargetCollection(convert_to_targets(train_data))\n",
    "mitchel_test = TargetCollection(convert_to_targets(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "mitchel_combined.no_targets_sentiment()": "{-1: 275, 0: 2306, 1: 707}",
     "mitchel_combined.ratio_targets_sentiment()": "{-1: 0.08, 0: 0.7, 1: 0.22}",
     "mitchel_test.no_targets_sentiment()": "{-1: 83, 0: 692, 1: 212}",
     "mitchel_test.ratio_targets_sentiment()": "{-1: 0.08, 0: 0.7, 1: 0.21}",
     "mitchel_train.no_targets_sentiment()": "{-1: 192, 0: 1614, 1: 495}",
     "mitchel_train.ratio_targets_sentiment()": "{-1: 0.08, 0: 0.7, 1: 0.22}"
    }
   },
   "source": [
    "The dataset has now been split with respect to the class labels so each class label is represented equally in the train and test splits which can be shown here:\n",
    "\n",
    "Train Data ratio: **{{mitchel_train.ratio_targets_sentiment()}}**\n",
    "Train Data raw values: **{{mitchel_train.no_targets_sentiment()}}**\n",
    "\n",
    "Test Data ratio: **{{mitchel_test.ratio_targets_sentiment()}}**\n",
    "Test Data raw values: **{{mitchel_test.no_targets_sentiment()}}**\n",
    "\n",
    "Original Data ratio: **{{mitchel_combined.ratio_targets_sentiment()}}**  \n",
    "Original Data raw values: **{{mitchel_combined.no_targets_sentiment()}}**\n",
    "\n",
    "We now save the data to XML file format which is the same as the SemEval data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data.semeval_14(full_path(read_config('mitchel_train')), mitchel_train)\n",
    "write_data.semeval_14(full_path(read_config('mitchel_test')), mitchel_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
